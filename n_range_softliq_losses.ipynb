{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook downloads the latest user snapshot data from the crvUSD subgraph and processes it to find the losses from soft liquidation in different band ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first portion of code manages the downloading and saving the data as a CSV called user_snapshots.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pluck\n",
    "import pandas as pd\n",
    "\n",
    "# crvusd subgraph url\n",
    "subgraph_url = 'https://api.thegraph.com/subgraphs/name/convex-community/crvusd'\n",
    "\n",
    "# function to get a portion of user data\n",
    "def get_user_data(skip_snapshots=0, skip_user_states=0):\n",
    "  query = f\"\"\"\n",
    "  {{\n",
    "    snapshots(first: 1000, skip: {skip_snapshots}, where: {{userStateSnapshot: true}}) {{\n",
    "      basePrice\n",
    "      oraclePrice\n",
    "      activeBand\n",
    "      userStates (first: 1000, skip: {skip_user_states}) {{\n",
    "        collateral\n",
    "        stablecoin\n",
    "        n\n",
    "        n1\n",
    "        n2\n",
    "        debt\n",
    "        depositedCollateral\n",
    "        health\n",
    "        loss\n",
    "        lossPct\n",
    "        timestamp\n",
    "        user {{\n",
    "          id\n",
    "        }}\n",
    "      }}\n",
    "      market {{\n",
    "        id\n",
    "        collateralName\n",
    "      }}\n",
    "    }}\n",
    "  }}\n",
    "  \"\"\"\n",
    "  frame, = pluck.execute(query, column_names=\"short\", url=subgraph_url)\n",
    "  return frame\n",
    "\n",
    "# function to get all user data\n",
    "def fetch_all_user_data():\n",
    "    skip_snapshots = 0\n",
    "    snapshot_df = pd.DataFrame()\n",
    "    \n",
    "    while True:\n",
    "        skip_user_states = 0\n",
    "        \n",
    "        while True:\n",
    "            print(f\"skip snapshots: {skip_snapshots}, user states: {skip_user_states}, snapshot_df length: {snapshot_df.shape[0]}\")\n",
    "            data = get_user_data(skip_snapshots, skip_user_states)\n",
    "            if(data.shape[1] == 17) and not data.isin(snapshot_df).all().all():\n",
    "                snapshot_df = pd.concat([snapshot_df, data], ignore_index=True)\n",
    "                skip_user_states += 1000\n",
    "            elif skip_user_states == 0:\n",
    "                return snapshot_df\n",
    "            else:\n",
    "                break\n",
    "        skip_snapshots += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch all user data\n",
    "data = fetch_all_user_data()\n",
    "\n",
    "# clean, make types correct and rename columns\n",
    "data = data.dropna()\n",
    "columns_to_int = ['activeBand', 'n', 'n1', 'n2', 'timestamp']\n",
    "columns_to_float = [\n",
    "    'basePrice', 'oraclePrice', 'collateral', 'stablecoin', 'debt',\n",
    "    'depositedCollateral', 'health', 'loss', 'lossPct'\n",
    "]\n",
    "data[columns_to_int] = data[columns_to_int].astype(int)\n",
    "data[columns_to_float] = data[columns_to_float].astype(float)\n",
    "data = data.rename(columns={'id': 'marketId', 'user.id': 'user'})\n",
    "\n",
    "# create some columns\n",
    "data['softLiq'] = data['activeBand'] >= data['n1']\n",
    "data['collateralUsd'] = data['collateral'] * data['oraclePrice']\n",
    "\n",
    "# save to csv\n",
    "data.to_csv(\"user_snapshots.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This portion calculates the losses from the user snapshots.  If you have already downloaded the snapshots, you can just run the notebook from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# if you want the data, run the get_data.ipynb notebook first, this will pull all the latest snapshots.\n",
    "data = pd.read_csv('user_snapshots.csv')\n",
    "\n",
    "# find the portion of collateral in crvUSD and the collateral token as a percentage\n",
    "data['collateralPct'] = data['collateralUsd']/(data['collateralUsd']+data['stablecoin'])*100\n",
    "data['stablecoinPct'] = 100-data['collateralPct']\n",
    "\n",
    "# currently softLiq column is True even when under softliq, let's create new columns to show the real soft liquidation\n",
    "data['under_softLiq'] = data['collateralPct'].eq(0)\n",
    "data['real_softLiq'] = (~data['under_softLiq']) & (data['softLiq'])\n",
    "\n",
    "# find the loan to value ratio\n",
    "data['ltv'] = data['debt']/(data['collateralUsd']+data['stablecoin'])*100\n",
    "\n",
    "# sort the data by user, marketId and timestamp\n",
    "data = data.sort_values(by=['user', 'marketId', 'timestamp']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The soft-liquidation loss statistics are misleading in the current form.  E.g. if a user loses 20% of their collateral\n",
    "# and then pays back most debt and withdraws 80% collateral the statistics will say the user lost 100% of their collateral\n",
    "\n",
    "# Let's calculate the loss per day while the user is in soft liquidation.  We will remove time periods where the user\n",
    "# did an action e.g. deposited or withdrew collateral, paid back debt or borrowed more.\n",
    "\n",
    "# create a lossPctPerDay column which counts the % a user lost between snapshots standardized to a day\n",
    "data['lossPctPerDay'] = 0\n",
    "\n",
    "# count the times a user changes their collateral and debt\n",
    "data['debtActions'] = 0\n",
    "\n",
    "# count the days a user is in soft liquidation\n",
    "data['softLiqDays'] = 0\n",
    "\n",
    "\n",
    "# need to iterate through data to get the above data.  This is slow but works.\n",
    "i = 0\n",
    "length = len(data)\n",
    "\n",
    "while i < length:\n",
    "\n",
    "    # get the current row data\n",
    "    row = data.iloc[i]\n",
    "    loan_id = row['user'] + row['marketId'] + str(row['depositedCollateral'])\n",
    "    collat_value = (row['collateralUsd'] + row['stablecoin']) / row['oraclePrice']\n",
    "    debt = row['debt']\n",
    "    timestamp = row['timestamp']\n",
    "    \n",
    "    # it the current loan is the same as the previous loan, ie. same user, marketId and depositedCollateral\n",
    "    # then we can calculate the lost value and log it if they didn't change their debt\n",
    "    if i > 0 and prev_loan_id == loan_id:\n",
    "\n",
    "        # lost value is how much the user lost between snapshots in their collateral e.g., WETH\n",
    "        lost_value = prev_collat_value - collat_value\n",
    "        time_days_diff = (timestamp - prev_timestamp) / 86400\n",
    "\n",
    "        # if the debt changed by more than 2% then we log it as an action\n",
    "        if prev_debt > debt * 1.02 or prev_debt < debt * 0.98:\n",
    "            data.at[i, 'debtActions'] += 1\n",
    "        \n",
    "        # else we log the lost value and the time between snapshots\n",
    "        elif lost_value > 0:\n",
    "            lossPctPerDay = lost_value / prev_collat_value / time_days_diff\n",
    "            data.at[i, 'softLiqDays'] = time_days_diff\n",
    "            data.at[i, 'lossPctPerDay'] = lossPctPerDay\n",
    "\n",
    "    # set the previous values to the current values\n",
    "    prev_collat_value = collat_value\n",
    "    prev_loan_id = loan_id\n",
    "    prev_timestamp = timestamp\n",
    "    prev_debt = debt\n",
    "\n",
    "    # print progress every 10,000 rows\n",
    "    if i % 10000 == 0:\n",
    "        print(f\"{i / length * 100}%\")\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get real soft liquidation subset of data\n",
    "softLiqData = data.loc[data['real_softLiq']].copy()\n",
    "\n",
    "# create bins for the number of bands a user chose\n",
    "bins = [3, 9, 19, 35, 50]\n",
    "labels = ['4-9', '10-19', '20-35', '36-50']\n",
    "softLiqData.loc[:, 'n_range'] = pd.cut(softLiqData['n'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# group the data by the number of bands a user chose\n",
    "sl_n_stats = softLiqData.groupby(['n_range']).agg({\n",
    "    'timestamp': 'count',\n",
    "    'lossPctPerDay': ['min', 'median', 'mean', 'std', 'max'],\n",
    "    'softLiqDays': 'sum'\n",
    "}).reset_index(drop=False)\n",
    "\n",
    "# rename the columns and save to csv\n",
    "sl_n_stats.columns = ['n_range', 'entries', 'lossPctDay_min', 'lossPctDay_median', 'lossPctDay_mean', 'lossPctDay_std', 'lossPctDay_max', 'softLiqDays']\n",
    "sl_n_stats.to_csv('grouped_soft_liq_stats.csv', index=False)\n",
    "print(sl_n_stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
